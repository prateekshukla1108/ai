{"Installing-Dependencies":{"title":"How to setup WSL and Python for fastAI","links":[],"tags":[],"content":"How to install WSL in windows\nWSL implies for Windows Subsystem for Linux (WSL), where you can run a Linux environment on your Windows machine without the hassle of dual-booting or setting up a virtual machine. This guide will take you through the steps to install WSL, focusing on WSL 2, which is faster, better, and more compatible than its predecessor.\nThe basic prerequisits are that You need Windows 10 version 2004 or higher (Build 19041 and above) or Windows 11. If you’re still rocking an older version, it might be time for an upgrade—like swapping out your flip phone for a smartphone!\nIf you get any error related to virtualization. You need to have virtualization enabled which you can do by enabling Hyper V in your device\nStep 1: Enable WSL:\nOpen PowerShell as Administrator: Search for “PowerShell” in the Start menu. Right-click on it and select “Run as administrator”. If you get intimidated by the black screen don’t panik you are inside terminal which helps us to execute commands. More on it later!\nRun the Installation Command: In the PowerShell window, type the following command and press Enter\nwsl --install\nThis command will enable all the necessary features for WSL, download the Linux Kernel, and install Ubuntu as your default distribution. A restart may be required.\nIf ubuntu is not your cup of tea then you can choose another distribution by\nwsl --install -d Debian\nfor debian\nStep 2: Make a user account\nAfter installation, launch your installed Linux distribution from the Start menu\nYou’ll be prompted to create a username and password. Choose wisely this is your secret identity! Remember, while typing your password, nothing will appear on the screen; this is normal behavior in Linux. It’s not broken; it’s just shy.\nStep 4: Update the system\nNow that you’ve got your Linux environment set up, let’s make sure it’s up to date: you can do that by just typing\nsudo apt update\nsudo apt upgrade\nTerminal and CLI\nOkay so now as you are staring at that black window let’s talk a little bit about terminal and cli\nA terminal is essentially a user interface that allows you to interact with your computer using text-based commands.\nCLI or command line interface is the way we interact with the computer in text format.\nNow by now 2 questions will arise in your mind by now -\n\n\nWhy are we using WSL why not use windows\nThe programs we will be using are much more easier to operate on Linux than windows, you can modify linux in many ways in order to make your life easier than it is in windows.\n\n\nWhy are we using CLI why not use the graphical apps which are more beautiful ?\nOnce you get the hang of it, typing commands can be faster than clicking through menus. You can write scripts to automate repetitive tasks saving time and effort.\n\n\nSome basic definitions and tools handy in cli\nSo here are some popular commands and definitions which we need to keep in mind while we use CLI\ndirectory - for most cases directory is your ‘folder’ we can store different files and directories in a directory.\nls : Lists the contents of a directory. Use options like -a for hidden files or -l for detailed information.\ncd [directory]: Changes the current directory to the specified one. Use cd .. to go back\nmkdir [dirname]: Creates a new directory with the specified name.\nrm -rf [dirname] : Removes a directory and everything in it. This is done without confirmation so know what you are doing\nInstalling python and other dependencies\nNow if you type python on your terminal you will see that some application is getting activated.\nNow the python that has been activated is not the python we are gonna use. This python is used by the system to run stuff. We are gonna something called miniforge for python.\nHere are the steps in setting up miniforge in your system -\nStep 1: Install wget in you system\nyou can do so by executing -\nsudo apt install wget\nStep 2: Download the setup script for miniforge\nyou can do that by following these commands\nfor x86_64\nwget github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nfor arm\nwget github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh\nthis will download the Miniforge installer in your WSL\nStep 3: Install miniforge in your WSL\nexecute the following command -\nFor x86_64\nbash Miniforge3-Linux-x86_64.sh\nFor Arm\nbash Miniforge3-Linux-aarch64.sh\nA simple setup will appear which will ask you to accept the licence agreement\nThen a prompt will appear telling you that setup will install miniforge in your home directory say yes to it\nAnd then miniforge will install not only python but a whole bunch of libraries which will come handy to us later.\nrestart you shell by executing bash to make it initialize miniforge\nStep 4: Agree to initialize it\nIt will ask you if you want to initialize it whenever you start your machine and say yes to it.\nWhat it will do is that it will execute python everytime we launch wsl\nMamba and conda\nMamba and Conda are both powerful tools used for package and environment management in Python. They install everything we need for python and help us create virtual environments. This brings us to -\nStep 5: Enable virtual environment\nA virtual environment is a self-contained directory that allows you to manage dependencies for different projects without stepping on each other’s toes. It will help seperate python we need with the system python\nTo create a virtual environment just execute:\nmamba create -n fastai_env python=3.9\nthis will create a python virtual environment\nbut that’s not all we also need to activate it for it to work this is done by executing\nmamba activate fastai_env\n\nA pro tip - You can activate the virtual environment everytime you want by putting it in your .bashrc file. You can do that by\n\nnano .bashrc\nthis opens a text editor which we will use to edit files. Edit it by adding mamba activate fastai_env at the end of the file.\nthen press ctrl + x to exit and y to save the file.\nStep 6: Installing ipython and jupyter lab\nNeed for these tools\nIf you’re ready to kick your AI game up a notch, you need to get cozy with IPython, JupyterLab, nbdev. We have so many good reasons to use these tools\nWe will use Ipython because\n\nIt helps us to display media like Images, Videos etc\nIPython includes special commands (prefixed with % or %%) that allow you to perform tasks like timing execution or running shell commands seamlessly.\nWith improved tracebacks and debugging capabilities, it makes troubleshooting easier.\n\nWe will use Jupyter Lab because -\n\n\nMulti document UI - Open multiple notebooks, text files, and terminals all at once. You can juggle your projects too. No more switching tabs.\n\n\nExtensions Galore: Want to customize your experience? JupyterLab supports extensions that let you add new features or integrate with other tools. It’s like dressing up your notebook in the latest fashion make it yours\n\n\nInteractive Widgets: Create interactive visualizations and controls right in your notebooks. Want to tweak parameters on the fly? Just slide those sliders.\n\n\nnbdev is Important because\n\nLiterate Programming: Write code, tests, and documentation together in Jupyter notebooks, enhancing readability and maintainability.\nAutomatic Documentation: Generate up-to-date documentation directly from your notebooks, streamlining the process of creating and maintaining libraries.\nIntegrated Testing: Write and run unit tests within your notebooks, ensuring code quality with automatic execution during builds and CI/CD processes.\n\nBasically they help is making the experiance smoother for the journey.\nInstallation\nHere is the command to install these tools -\nmamba install ipython jupyterlab nbdev\nStep 7: Install pytorch\nPyTorch is a powerful and flexible tool for deep learning and machine learning projects. Here are some of its features\n\nDynamic Computation Graphs: Allows changes to the model on-the-fly, making debugging easier.\nTensor Operations: Supports efficient tensor computations with GPU acceleration for faster processing.\nUser-Friendly: Intuitive and Pythonic interface, great for beginners and experienced users alike.\nRich Ecosystem: Includes libraries for building neural networks and optimization, simplifying model development.\nStrong Community: Extensive documentation and active community support for learning and troubleshooting.\n\nInstallation\nHere is how to get it installed -\n\nFor devices with Nvidia GPU - if your device have an Nvidia GPU then you can install pytorch with CUDA support by executing following command -\n\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\nYou will also need to install the cuda toolkit for pytorch to work. You can do it by executing -\n\nwget developer.download.nvidia.com/compute/cuda/12.6.2/local_installers/cuda_12.6.2_560.35.03_linux.run\n\nThen run\nsudo sh cuda_12.6.2_560.35.03_linux.run\n\nFor devices with integrated graphics - If you are poor student like me and have device with integrated graphics then you should install pytorch by using following command -\n\nmamba install pytorch torchvision torchaudio cpuonly -c pytorch\nStep 8: Install FastAI\nFast AI will be the main library we will be working with. It is designed to make deep learning accessible to everyone, regardless of their coding experience. It is is built on top of PyTorch designed to make the complex realm of artificial intelligence as approachable as your favorite recipe for instant noodles.\nInstallation\nTo install it just execute this command in your terminal -\nmamba install -c fastai fastai\nAnd you have successfully installed the tools required for the course.\nOptional stuff\nVim\nVim is the most supereme cli text editor that one can use in linux. Here is how to install and use it\n\n\nOpen WSL Terminal:\n\nLaunch your WSL terminal.\n\n\n\nUpdate Package List:\n\nBefore installing any software, update the package list by running:\nsudo apt update\n\n\n\n\nInstall Vim:\n\nInstall Vim by executing the following command:\nsudo apt install vim -y\n\nThis command retrieves and installs Vim along with its necessary components.\n\n\n\nLaunching Vim:\n\nTo create or edit a file, use the command:\nvim filename.txt\n\nReplace filename.txt with your desired file name. If the file does not exist, Vim will create it.\n\n\n\nBasic Navigation and Editing:\n\nUpon opening a file, you start in Normal mode. Press i to switch to Insert mode, where you can type text.\nTo return to Normal mode, press Esc.\nYou can go up down left and right in the document by either using arrow keys or using h,j,k,l keys (right,down,up,left).\n\n\n\nSaving and Exiting:\n\nTo save changes, type :w and press Enter.\nTo exit Vim, type :q and press Enter. If you want to save and exit simultaneously, type :wq.\n\n\n\nRanger\nRanger is a cli file manager which you can use to navigate through files easily.\nTo install and use Ranger, a VIM-inspired file manager, in Windows Subsystem for Linux (WSL), follow these detailed steps:\nInstallation Steps\n\n\nOpen WSL Terminal:\n\nLaunch your WSL terminal\n\n\n\nInstall Prerequisites:\n\nUpdate the package list and install the necessary packages (make, git, and vim) by running:\nsudo apt update\nsudo apt install make git vim -y\n\n\n\n\nInstall Ranger:\nsudo apt install ranger -y\n\n\nConfigure Ranger:\n\nRun Ranger once to create the configuration directory:\nranger\n\n\n\n\nUsing Ranger\n\n\nLaunching Ranger:\n\nStart Ranger by typing:\nranger\n\n\n\n\nInterface Overview:\n\nThe interface is divided into three columns:\n\nLeft Column: Displays the parent directory.\nMiddle Column: Shows contents of the current directory.\nRight Column: Provides a preview of the selected file or folder.\n\n\n\n\n\nBasic Navigation:\n\nUse the following keys to navigate:\n\nArrow keys or h, j, k, l for left, down, up, and right respectively.\nEnter to open a file or directory.\nq to quit.\n\n\n\n\n\nCopying, Pasting, and Deleting Files\n\n\nCopying Files:\n\nTo copy a file or directory, navigate to it and press yy (yank).\nTo copy multiple files, select them using Space and then press yy.\n\n\n\nPasting Files:\n\nNavigate to the destination directory and press p to paste the copied files.\n\n\n\nDeleting Files:\n\nTo delete a file or directory, navigate to it and press dd (delete).\nConfirm the deletion when prompted.\n\n\n"},"Lesson-1-Making-a-Cyclist-recogniser":{"title":"Lesson 1: Vision models and Fundamentals","links":[],"tags":[],"content":"The evolution of Machine Learning has transformed from a concept once deemed nearly impossible to a technology that is now easily accessible and widely utilized. It was considered so ridiculous in the early days that people joked about it. Here is one example:\n\nAbove is an xkcd comic which shows how people joked about it. The good news is that we are going to make a computer vision model in this lesson today. So get excited!\nThe Evolution of Neural networks\nBefore Neural Networks\nIn the era before neural networks, people used a lot of workforce to identify images, then many mathematicians and computer scientists to process those images and create separate features for each one of them. After a lot of time and processing, they would fit it into a machine learning model. It became successful, but the problem was that making these models took a lot of time and energy, which was inefficient and tedious.\nThe first neural network\nBack in 1957 a neural network was described as something like a program. So in a traditional program we have some inputs and then we put them in program which have functions, conditionals, loops etc and these give us the result.\nDeep learning models\nIn deep learning the program is replaced by Model and we now also have weights(Also called parameters) with inputs. The model is not anymore a bunch of conditionals and loops and things. In case of a neural network it is a mathematical function which takes the inputs, multiplies them together by the weights and adds them up. And it does that will all the sets of inputs. And thus a neural network is formed\nNow a model will not do anything useful unless these weights are carefully chosen, so we start by these weights being random. Initially these networks don’t do anything useful.\nWe then take the inputs and weights put them in our model and get the results. The we decide how good they are, this is done by a number called loss. Loss describe how good the results are, think of it as something like accuracy. After we get loss we use it to update our weights and then repeat this process again and again, we get better and better results.\nOnce we do this enough times we stop putting inputs and weights and replace it with inputs and get some outputs.\nHow Modern Neural Networks Work\nWith modern neural network methods, we don’t teach the model features; we make them learn features. It is done by breaking the image into small parts and assigning them features (often called layer 1 features). After doing this for many images, we combine them to create more advanced features. So we train the basic neural network and make it a more advanced neural network, creating a kind of feature detector that finds the related features.\nCoding these features would be very difficult, and many times you wouldn’t even know what to code. This is how we make neural networks more efficient by not making them by code but by making them learn.\nMisconceptions About Deep Learning\nAs we saw earlier, to train a computer vision model, we didn’t need expensive computers, we didn’t need very high-level math, and we didn’t need lots of data. This is the case with much of deep learning which we will learn. There will be some math that will be needed but mostly, either we will teach you the little bits, or we will refer you to some resources.\nPyTorch vs TensorFlow\nIn recent years, PyTorch is increasingly used in research while TensorFlow is declining in popularity. The library which is used in research is more likely to be used in industry; therefore, we will be using PyTorch for learning Deep Learning.\nAnother thing to note is that sometimes PyTorch uses a lot of code for some really basic tasks, and this is where fastai comes into play. The operations which are really lengthy to implement in PyTorch can be done with very few lines of code with fastai. This is not because PyTorch is bad but because PyTorch is designed so that many good things can be built on top of it.\nThe problem with having lots of code is that it increases the chances of mistakes. In fastai, the code you don’t write is code that the developers have found best practices for and implemented for you.\nJupyter Notebook\nJupyter notebook is a web-based application which is widely used in academia and teaching, and it is a very powerful tool to experiment, explore, and build with.\nNowadays, most people don’t run Jupyter notebooks on their own local machines but on cloud servers. If you go to course.fast.ai, you can see how to use Jupyter and cloud servers. One of the good ones is Kaggle. Kaggle doesn’t only have competitions but also has cloud servers where you can train neural networks. You can learn more about it at course.fast.ai/Resources/kaggle.html.\nMaking a Traffic Recognizer\nLet’s say you want to make a self-driving car. A big challenge for it would be to identify between cyclists and pedestrians, so we are going to do that now. We are going to make a computer vision model that can differentiate between cyclists and pedestrians.\nImport Statements\nfrom duckduckgo_search import DDGS\nfrom fastcore.all import *\nimport time\nimport json\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\nThese lines import necessary libraries:\n\nDDGS: DuckDuckGo search API for finding images\nfastcore: Utility functions for deep learning\nfastdownload: For downloading files from URLs\nfastai: Deep learning library built on PyTorch\ntime: For time-related operations\n\nNote: You might get an error for duckduckgo_search while executing this part. Don’t panic - just go to the console and execute:\npip install duckduckgo-search\nThis will install duckduckgo-search in your notebook.\nImage Search Function\ndef search_images(keywords, max_images=400):\n    return L(DDGS().images(keywords, max_results=max_images)).itemgot(&quot;image&quot;)\nThis function:\n\nTakes search keywords and maximum number of images\nUses DuckDuckGo to search for images\nReturns a list of image URLs\nL() creates a fastai list\nitemgot(&quot;image&quot;) extracts just the image URLs from the search results\n\nInitial Test Downloads\nurls = search_images(&quot;pedestrians&quot;, max_images=1)\nprint(urls[0])\ndest = &quot;pedestrians.jpg&quot;\ndownload_url(urls[0], dest, show_progress=False)\nim = Image.open(dest)\nThis section:\n\nSearches for one pedestrian image\nDownloads it as ‘pedestrians.jpg’\nOpens it to verify the download worked\n\nNow we all know that computers don’t understand images, but the good news is that computers can understand numbers, and all images are made up of pixels which contain information about the brightness of red, green, and blue colors. So every picture is just a collection of numbers representing the amount of red, green, and blue in each pixel.\ndownload_url(\n    search_images(&quot;Cyclist&quot;, max_images=1)[0], &quot;cyclist.jpg&quot;, show_progress=False\n)\nImage.open(&quot;cyclist.jpg&quot;).to_thumb(256, 256)\n\nWe are now downloading the test image\nOur model will predict if this image is an image of cyclists\nCreates a 256x256 thumbnail version\n\nDataset Creation\nsearches = [&quot;Cyclists&quot;, &quot;Pedestrians&quot;]\npath = Path(&quot;pedestrians_or_cyclists&quot;)\nfor o in searches:\n    dest = path / o\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f&quot;{o} photo&quot;))\n    time.sleep(5)\n    resize_images(path / o, max_size=400, dest=path / o)\nThis loop:\n\nCreates directories for each category\nDownloads multiple images for each category\nAdds “photo” to search terms for better results\nWaits 5 seconds between searches to be polite to the search API\nResizes all images to a maximum size of 400 pixels\n\nImage Verification\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\nThese lines:\n\nCheck all downloaded images for corruption\nDelete any corrupt images\nCount how many images were removed\n\nDataLoader Creation\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method=&quot;squish&quot;)],\n).dataloaders(path, bs=10)\nThis creates a FastAI DataBlock with:\n\nImage inputs and category labels\n80/20 train/validation split\nDirectory names as labels\nImages resized to 192x192\nBatch size of 10 (Number of images shown to check the data)\n\nLet’s discuss these one by one:\nThe first thing that we tell fastai is what kind of input we have. This is defined by blocks. Here the input is Image, therefore we set blocks to ImageBlock. To find all the inputs to our model, we run get_image_files (function which returns all image files in a path).\nWe used the same function earlier to remove all the corrupted images.\nNow it is important to put aside some data to test the accuracy of our model. It is so critical to do so that fastai won’t let you train a model without that information. How much data is going to be set aside is determined by RandomSplitter. (In our case, we are randomly setting aside 20% of data for our validation set).\nNext, we tell fastai the way to know the correct label of the photo. We do this with the get_y function. This function tells it to take the label of the photo from its parent directory (the directory in which the photo is stored).\nAll computer vision architectures need all of the input to be the same size. By using item_tfms, we are resizing every image to be (192,192) each, and the method of resize is going to be ‘squish’. You can also crop it from the middle.\nWith the help of dataloaders, PyTorch can grab a lot of your data at one time, and this is done quickly using a GPU which can do a thousand things at once. So dataloader will feed the image model with a lot of data that is provided to it at once. We call these images a batch.\nbs=10 will show 10 images from the data it is feeding to the algorithm with labels.\nNote: You can learn more about these functions by going to the tutorials and documentation of fastai.\nModel Training\nHere we will train our model with resnet18. In fastai, a learner is something which combines our neural net and the data we train it with.\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\nThese lines:\n\nCreate a vision model using ResNet18 architecture. It’s like creating a baby AI, except it won’t keep you up at night (your debugging sessions will do that instead).\nFine-tune it for 3 epochs\nTrack error rate as the metric\n\nIt will usually take about a minute or two if you use only CPU, but it can take about 10 seconds if you train it on GPU. This difference is because someone has already trained it (pretraining) to recognize over 14 Million images over 20,000 different types, something called the ImageNet dataset. So you actually start with a network which can do a lot, and they made all these parameters available to download from the internet.\nlearn.fine_tune takes those pre-trained weights and adjusts them to teach the model the differences between your datasets and what it was originally trained for. This method is called fine tuning.\nYou can get different architectures from timm.fast.ai/\nPrediction\nis_cyclist, _, probs = learn.predict(PILImage.create(&#039;cyclist.jpg&#039;))\nprint(f&quot;This is a: {is_cyclist}.&quot;)\nprint(f&quot;Probability that it is cyclist: {probs[0]:f}&quot;)\nFinally:\n\nLoads and predicts on a test image\nPrints the predicted class\nShows the probability of the prediction\n\nDeep Learning Is Not Just for Image Classification\nIn above example we say the uses of deep learning for image classification. Now we will see another beautiful example. This is called Segmentation.\nSegmentation where we take photos and we color every pixel to identify different components of the image.\n# Import all functions and classes from fastcore and fastai.vision libraries\nfrom fastcore.all import *\nfrom fastai.vision.all import *\nThese lines import the necessary libraries. fastcore provides core utilities, while fastai.vision contains computer vision-specific functionality.\n# Download and extract the CAMVID_TINY dataset to a local path\npath = untar_data(URLs.CAMVID_TINY)\nThis downloads a small version of the Cambridge-driving Labeled Video Database (CamVid), which contains road scene images with pixel-level segmentation labels. untar_data downloads and extracts the dataset if not already present.\ndls = SegmentationDataLoaders.from_label_func(\n    path,                   # Path to the dataset\n    bs=8,                   # Batch size for training\n    fnames = get_image_files(path/&quot;images&quot;),  # Get list of all image files\n    label_func = lambda o: path/&#039;labels&#039;/f&#039;{o.stem}_P{o.suffix}&#039;,  # Function to find corresponding label file\n    codes = np.loadtxt(path/&#039;codes.txt&#039;, dtype=str)  # Load class names from codes.txt\n)\nThis creates a DataLoader specifically for segmentation tasks:\n\nbs=8 sets the batch size to 8 images\nget_image_files() gets all image files from the images directory\nThe label_func is a lambda function that maps each image file to its corresponding label file by adding ‘_P’ to the filename\ncodes.txt contains the names of segmentation classes (like ‘road’, ‘building’, etc.)\n\n# Create a U-Net model with ResNet34 backbone\nlearn = unet_learner(dls, resnet34)\nThis creates a U-Net architecture (common for segmentation tasks) using ResNet34 as the backbone. U-Net is particularly effective for semantic segmentation because it combines detailed spatial information with deep features.\n# Fine-tune the model for 8 epochs\nlearn.fine_tune(8)\nThis trains the model using transfer learning. It first trains the newly added U-Net layers while keeping the pretrained ResNet34 frozen, then fine-tunes the entire network for 8 epochs.\n# Display prediction results for up to 6 images\nlearn.show_results(max_n=6, figsize=(7,8))\nThis displays a grid showing the original images, their true segmentation masks, and the model’s predicted segmentation masks for up to 6 images. The figsize parameter sets the size of the display.\nTabular Analysis\nWe can also help analyze structured data like spreadsheets. Let’s start by breaking down each line of code and understanding what it does:\nfrom fastai.tabular.all import *\nThis line imports all the necessary functions and classes from fastai’s tabular module.\npath = untar_data(URLs.ADULT_SAMPLE)\nHere, we’re downloading and extracting a sample dataset called “Adult” that predicts whether someone makes over $50K per year. The untar_data function:\n\nDownloads the dataset if it’s not already present\nExtracts it from its compressed format\nReturns the path where the data is stored\n\ndls = TabularDataLoaders.from_csv(\n    path/&#039;adult.csv&#039;,\n    path=path,\n    y_names=&quot;salary&quot;,\n    cat_names = [&#039;workclass&#039;, &#039;education&#039;, &#039;marital-status&#039;,\n                 &#039;occupation&#039;, &#039;relationship&#039;, &#039;race&#039;],\n    cont_names = [&#039;age&#039;, &#039;fnlwgt&#039;, &#039;education-num&#039;],\n    procs = [Categorify, FillMissing, Normalize]\n)\nThis is where the magic begins! Let’s break this down:\n\nWe’re creating a DataLoader object that handles how we feed data to our model\npath/&#039;adult.csv&#039;: Specifies the CSV file containing our data\ny_names=&quot;salary&quot;: Indicates that “salary” is our target variable (what we’re trying to predict)\ncat_names: Lists our categorical columns (text or discrete data) like workclass, education etc\ncont_names: Lists our continuous numerical columns like age, final weight(a census data concept), education num(numerical encoding of education level). These are the columns which can take any real number\nprocs: Specifies the preprocessing steps:\n\nCategorify: Converts categorical variables into numbers\nFillMissing: Handles any missing values\nNormalize: Scales numerical data to a similar range\n\n\n\nlearn = tabular_learner(dls, metrics=accuracy)\nThis line creates our machine learning model:\n\ntabular_learner: Creates a neural network designed for tabular data\nmetrics=accuracy: Tells the model to track prediction accuracy during training\n\ndls.show_batch()\nThis displays a sample of our processed data, helping us verify that everything looks correct before training.\nThe beautiful thing about this function is that it uses type dispatch which is particularly used in a language called julia and it allows us to define functions that can adapt their behavior according to input types. Basically it will provide realistic data for age, fnlwgt etc\nlearn.fit_one_cycle(2)\nFinally, we train our model:\n\nWe don’t say fine tune model because for tables because every table of data is very different. So we just ‘fit’ the data.\nThe number 2 indicates we’ll train for 2 epochs (full passes through the data)\n“one_cycle” refers to the One Cycle Policy, a training technique that helps achieve better results faster\n\nWhy Tabular Deep Learning?\nYou might wonder why we’d use deep learning for tabular data when we have traditional methods like Random Forests or XGBoost. Here’s why:\nIt can help in financial predictions, risk assesment, sales forcasting etc\nCollaborative Filtering\nCollaborative filtering is the basis of most recommandation systems today. From your youtube recommandation to the recommandations you see in spotify.\nIt works by finding which users liked which products and then it use that to guess what other product smilar user liked and based on those smilar users what users might like.\nNote that here Similar Users don’t mean similar demographically but similar in sense that those people liked the same kind of product that you liked.\n1. Import the necessary modules:\nfrom fastai.collab import *\n\nThis imports everything from the fastai.collab module, which provides tools for collaborative filtering and recommendation systems.\n\n2. Download and extract sample data:\npath = untar_data(URLs.ML_SAMPLE)\n\nAgain we are downloading and extracting data here\nURLs.ML_SAMPLE points to a small sample dataset from the MovieLens dataset, commonly used in recommendation system experiments.\n\n3. Create data loaders for collaborative filtering:\ndls = CollabDataLoaders.from_csv(path/&#039;ratings.csv&#039;)\n\nCollabDataLoaders.from_csv reads a CSV file containing user-item interaction data (e.g., user ratings for movies) to create a data loader.\npath/&#039;ratings.csv&#039; specifies the path to the CSV file that contains the ratings dataset.\nThe dls object now holds the data loaders, which manage the data used for training and validation during the model’s training process.\n\n4. Display a sample batch of data:\ndls.show_batch()\n\nThis displays a sample batch of user-item-rating triplets (e.g., a user ID, a movie ID, and the user’s rating for the movie).\n\n5. Create a collaborative filtering model:\nlearn = collab_learner(dls, y_range=(0.5,5.5))\n\ncollab_learner initializes a collaborative filtering model based on a neural network architecture.\nThe dls object is passed to define the input data.\ny_range=(0.5, 5.5) specifies the range of the target values (ratings), helping the model output predictions in the expected range.\n\n6. Train the model with fine-tuning:\nlearn.fine_tune(10)\n\nfine_tune trains the model for a specified number of epochs—in this case, 10 epochs.\nThe model first uses a pre-trained embedding (if available) and then fine-tunes it on the given dataset.\n\n7. Display the results:\nlearn.show_results()\n\nThis method shows the predictions made by the model alongside the actual ratings from the test dataset.\nIt provides an intuitive way to evaluate the model’s performance by comparing predicted and actual ratings.\n\nThe Future of deep learning\nWe are still scratching the tip of the iceberg in the field of AI, particularly deep learning, despite its widespread adoption and heavy marketing. The advancements we’ve witnessed so far, though impressive, represent only the beginning of what’s possible. The landscape of AI is rich with pre-trained models, open-source frameworks, and affordable access to GPUs, making it easier than ever for individuals and organizations to learn, innovate, and expand within this domain.\nDeep learning thrives in areas where tasks require processing vast amounts of data to uncover patterns that humans often miss. If a task can be completed by a human in a short period, even if it requires significant expertise, deep learning models have shown remarkable capabilities to replicate and sometimes surpass human performance. Examples abound, from image and speech recognition to real-time language translation and game-playing strategies.\nHowever, the limitations of deep learning become evident in areas demanding prolonged logical reasoning, complex problem-solving, or tasks rooted in abstract thought. These tasks require an understanding of nuanced context, long-term planning, or forming new concepts beyond the data it has been trained on—areas where humans still hold a decisive advantage.\nDeep learning, at its core, is an excellent pattern recognizer but not an innate reasoner. While models like GPT and DALL-E can generate creative outputs, their reasoning capabilities are constrained by the structure and scope of their training data. They excel at interpolation within known data distributions but falter when extrapolating to scenarios far removed from their training domain.\nThe future of deep learning lies in addressing these limitations. Innovations such as integrating symbolic reasoning with neural networks, neuromorphic computing, and leveraging advances in unsupervised learning hold promise for overcoming the current bottlenecks. Moreover, as the field pushes towards AGI (Artificial General Intelligence), the convergence of deep learning with other disciplines like neuroscience, quantum computing, and systems biology may pave the way for breakthroughs that are currently unimaginable.\nRemember: The best time to start learning machine learning was yesterday. The second best time is now. So what are you waiting for? Let’s teach some computers to see! 🚀\n\nP.S. No neural networks were harmed in the making of this blog post. They were just mildly confused.\nNow go forth and may your loss curves be ever decreasing! 📉✨\nP.S. If your model starts predicting lottery numbers, please share them. For scientific purposes, of course."},"Lesson-2-data-augmentation-and-putting-models-on-production":{"title":"Lesson 2: Data Augmentation and Putting the model on production","links":[],"tags":[],"content":"This lesson is going to be about making a bear classifier and putting it into production. The project that we will be using will be a bear classifier and it will discriminate between black bears, Grizzly Bears, Teddy Bears. Also at last we will be also discussing on how to make things faster when using jupyter\n1. Setting Up Our Tools\nFirst, we need to import some special tools (we call them libraries) that will help us build our bear classifier:\nfrom duckduckgo_search import DDGS\nfrom fastcore.all import *\nimport time\nfrom fastai.vision.widgets import *\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\nWhat do these tools do?\n\nduckduckgo_search: Helps us find bear pictures on the internet\nfastai: A powerful library that makes AI easy to use\ntime: Helps us add delays when downloading images\nfastdownload: Makes downloading images easy\n\n2. Getting Our Bear Images\nNow, let’s create a function to search for bear images:\ndef search_images(keywords, max_images=400):\n    return L(DDGS().images(keywords, max_results=max_images)).itemgot(&quot;image&quot;)\nThis function is like having a personal assistant who can search the internet for bear pictures! We tell it what kind of bear we want, and how many pictures we need.\nLet’s test it by downloading one image of each bear type:\n# Download a grizzly bear image\nurls = search_images(&quot;grizzly bear&quot;, max_images=1)\ndownload_url(urls[0], &quot;grizzly.jpg&quot;, show_progress=False)\n \n# Download a teddy bear image\ndownload_url(search_images(&quot;Teddy Bear photos&quot;, max_images=1)[0], &quot;teddy.jpg&quot;, show_progress=False)\n \n# Download a black bear image\ndownload_url(search_images(&quot;Black Bears&quot;, max_images=1)[0], &quot;Black.jpg&quot;, show_progress=False)\n3. Organizing Our Data\nNow we need to create folders for our different types of bears:\nbear_types = &#039;grizzly&#039;, &#039;black&#039;, &#039;teddy&#039;\npath = Path(&#039;bears&#039;)\nsearches = [&quot;Grizzly Bear&quot;, &quot;Black Bear&quot;, &quot;Teddy bear&quot;]\n \n# Create folders and download multiple images\nfor o in searches:\n    dest = path / o\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f&quot;{o} photo&quot;))\n    time.sleep(5)  # Wait 5 seconds between searches\n    resize_images(path / o, max_size=400, dest=path / o)\nThis code:\n\nCreates a folder called “bears”\nInside it, creates three more folders: one for each bear type\nDownloads multiple images for each bear type\nResizes them to make sure they’re not too big\n\n4. Preparing Images for Training\nBefore we can train our AI, we need to check our images and prepare them:\n# Find all our image files\nfns = get_image_files(path)\n \n# Check for any broken images and remove them\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\nNow, we’ll create a DataBlock to handle images\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),  # We&#039;re working with images and categories\n    get_items=get_image_files,          # How to find our images\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),  # Split data into training and testing\n    get_y=parent_label,                 # Use folder names as labels\n    item_tfms=Resize(128))              # Resize all images to the same size\n5. Training Our AI Model\nNow we will be training our model with Resnet18\ndls = bears.dataloaders(path)\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)  \nWe can make this faster and more efficient by resizing or cropping the images. We can do this by 3 ways\nResizing\nBy default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details.\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1)\nPadding\nAlternatively, you can ask fastai to pad the images with zeros (black). This fills the empty spaces with black.\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#039;zeros&#039;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1)\nAll of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to perform recognition. For instance, if we were trying to recognize breeds of dog or cat, we might end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model and results in a lower effective resolution for the part of the image we actually use.\nRandomResizedCrop\nInstead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\nIn fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn’t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.\nHere is the exmaple -\n \nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True)\n \nThe batch_tfms=aug_transforms(mult=2) applies data augmentation techniques, effectively doubling the number of training samples by creating variations of the original images through transformations like rotation, flipping, or color adjustments.\n6. Checking How Well It Works\nFinally, let’s see how well our AI does:\ninterp = ClassificationInterpretation.from_learner(learn)\nThis creates an interpretation object from your trained model (the learn object). It’s a crucial tool for analyzing how your model performs, containing information about predictions, actual labels, losses, and other metrics.\ninterp.plot_confusion_matrix()\nThis creates a confusion matrix visualization - a table showing:\n\nRows: Actual classes\nColumns: Predicted classes\nEach cell: Number of predictions This helps you see where your model is getting confused. For example, if you’re classifying cats and dogs, it shows how many cats were correctly identified as cats, how many cats were mistakenly labeled as dogs, etc.\n\n interp.plot_top_losses(5, nrows=1)\nThis shows the 5 images where your model performed worst (had highest loss):\n\nEach image is displayed with:\n\nThe actual label\nThe predicted label\nThe loss value\nThe probability the model assigned to its prediction This is invaluable for understanding what types of images your model struggles with\n\n\n\nExtra: Cleaning Up Our Dataset\nIf we want to improve our model, we can use this tool to review and clean our dataset:\ncleaner = ImageClassifierCleaner(learn)\nThis creates an interactive tool (works in Jupyter notebooks) that lets you:\n\nBrowse through your dataset\nFlag incorrect labels\nDelete problematic images\nRemove duplicates It’s especially useful when you discover through the interpretation tools that your dataset has issues like mislabeled images or poor quality samples.\n\nYou can find which images are bad and you can keep it or remove it or put it in the correct catagory.\nHuggingface and Gradio\nAfter training the AI model you might want to deploy it somewhere and that is where huggingface and gradio comes into picture.\nWhat is Gradio?\nGradio is a Python library that allows you to quickly create customizable user interfaces for machine learning models. With just a few lines of code, you can turn your model into an interactive web application, making it easy for others to test and use your model without needing any programming skills.\nWhat is Hugging Face?\nHugging Face is a popular platform that provides state-of-the-art pre-trained models for natural language processing (NLP), computer vision, and more. Their library, transformers, offers a wide range of models that you can use directly in your applications.\nStarting Up\nTo start firstly you need to take your model. You can do this by using -\nlearn = load_learner(&#039;export.pkl&#039;)\nThis will export your model into a file named export.pkl\nThen you would need to install gradio and transformers. You can do it by using pip or conda\nconda install `gradio transformers`\nCreating a blog post for beginners about using Gradio with Hugging Face can be an exciting way to introduce them to the world of machine learning and user interfaces. Below is a simplified and engaging blog post inspired by the provided content.\nSetting up huggingface\nGo to huggingface.co/spaces and create a new space. You will need to sign up to make a new space. After signing up, you will be prompted with this kind of interface:\nFill in the relevant details and create your Hugging Face space. After you create it, you will be greeted by a beautiful page like this:\nYou need to generate a token, clone the repository on your machine, edit a file, and push it back to the Hugging Face platform.\nIf you feel intimidated by all this jargon, consider learning how to use Git or GitHub through resources available online.\nStep 1: Generate Your Access Token\nBefore cloning your space, you need to generate an access token. This token allows you to authenticate and push changes to your repository. To generate a token:\n\nNavigate to your account settings on Hugging Face.\nClick on “Access Tokens” in the sidebar.\nCreate a new token with write access.\n\nStep 2: Clone Your Repository\nOnce you have your access token, open your terminal and navigate to the directory where you want to clone your space. Use the following command:\ngit clone https://&lt;your-username&gt;:&lt;your-access-token&gt;@huggingface.co/spaces/&lt;your-space-name&gt;\nReplace &lt;your-username&gt;, &lt;your-access-token&gt;, and &lt;your-space-name&gt; with your actual Hugging Face username, the generated token, and the name of your space respectively. This command will create a local copy of your space where you can make changes.\nStep 3: Edit Your Files\nNavigate into the cloned directory:\ncd &lt;your-space-name&gt;\nOpen the folder in your preferred code editor (like Visual Studio Code). You will typically find a few default files such as .gitattributes and README.md.\nTo create functionality for your space, you’ll need to add an app.py file. This is where you’ll write the code for your application.\nStep 4: Create Your Application\nIn app.py, you can start coding your application. Here is the default text which comes with gradio\nimport gradio as gr\n \ndef greet(name):\n    return &quot;Hello &quot; + name + &quot;!!&quot;\n \ndemo = gr.Interface(fn=greet, inputs=&quot;text&quot;, outputs=&quot;text&quot;)\ndemo.launch()\nThis code snippet sets up a basic Gradio interface that allows users to upload an image and receive a classification result.\nStep 5: Prepare for Deployment\nBefore pushing your changes back to Hugging Face, create a .gitignore file in your project directory. This file helps prevent unnecessary files from being uploaded. Here’s an example of what to include in .gitignore:\n__pycache__/\n*.pyc\nvenv/\n\nStep 6: Push Your Changes\nAfter editing and saving your files, it’s time to push them back to Hugging Face. First, check what changes are staged for commit:\ngit status\nIf everything looks good, add your changes:\ngit add .\nThen commit them with a message:\ngit commit -m &quot;Initial commit of my Hugging Face Space&quot;\nFinally, push the changes:\ngit push origin main\n\nNote: If you are afraid of cli then you can also use github desktop which is beautiful GUI version to manage git repositories\n\n\nNote: You can also create your app.py file from your browser\n\nStep 7: View Your Deployed Space\nAfter pushing successfully, navigate back to your Hugging Face Space URL (e.g., huggingface.co/spaces/&lt;your-username&gt;/&lt;your-space-name&gt;). You should see your application live! It may take a moment for the deployment process to complete.\nCongratulations! You have successfully set up and deployed your first Hugging Face Space. Now you can share it with friends or colleagues and explore adding more features or models.\nSetting up environment for Python\nHere we will not be discussing about setting up our python environment for development. To learn about that you need to see the guide here - prateekshukla1108.github.io/fastai/posts/Installing-Dependencies.html"},"index":{"title":"Welcome to My Quartz Blog","links":["posts/","projects/","notes/"],"tags":[],"content":"Recent Posts\nHere are some of my latest writings:\n&lt;Component.RecentNotes { title: “Recent Posts”, limit: 5, showTags: true, linkToMore: “/tags” } /&gt;\nCategories\nExplore my content by category:\n\nposts\nprojects\nnotes\n\nAbout\nWelcome to my blog! Here, I share insights on various topics ranging from technology to personal development. Feel free to browse through my posts and connect with me.\nContact\nYou can reach me at [your-email@example.com]."}}